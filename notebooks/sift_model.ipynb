{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningDataModule\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Subset, DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_lightning import LightningDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.baseline import BaselineClf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIFTClf(pl.LightningModule):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.accuracy = torchmetrics.Accuracy()\n",
    "        self.in_channels = in_channels\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, 16, kernel_size=3, stride=2, padding=1, bias=False), # 64x64\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias=False), # 32x32\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=False), # 16x16\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False), # 8x8\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False), # 4x4\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, bias=False), # 2x2\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(512, 64, bias=False),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch \n",
    "        labels = torch.squeeze(labels)\n",
    "        out = self(images)         \n",
    "        loss = F.cross_entropy(out, labels) \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch \n",
    "        labels = torch.squeeze(labels)\n",
    "        out = self(images)                    \n",
    "        loss = F.cross_entropy(out, labels)   \n",
    "        _, preds = torch.max(out, dim=1)\n",
    "        self.accuracy(preds, labels)\n",
    "        self.log('Validation Accuracy: ', self.accuracy)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, labels = batch \n",
    "        labels = torch.squeeze(labels)\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)   \n",
    "        _, preds = torch.max(out, dim=1)\n",
    "        self.accuracy(preds, labels)\n",
    "        self.log('Test Accuracy: ', self.accuracy)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIFTDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.__len__()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, feature = self.data[idx]\n",
    "        feature = np.expand_dims(feature, axis=0)\n",
    "        return torch.tensor(feature, dtype=torch.float), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIFTDataModule(LightningDataModule):\n",
    "    def __init__(self, batch_size=32, num_workers=0):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        print(\"SIFT DataModule: setup...\")\n",
    "        data_path = \"../data/images\"\n",
    "\n",
    "        data = []\n",
    "        for file in os.listdir(data_path + \"/yes\"):\n",
    "            img = cv2.imread(data_path + \"/yes/\" + file)\n",
    "            sift = cv2.SIFT_create()\n",
    "            kp, desc = sift.detectAndCompute(img, None)\n",
    "            features = np.resize(desc, (256, 256))\n",
    "            label = np.array([1])\n",
    "            data.append([label, features])\n",
    "\n",
    "        for file in os.listdir(data_path + \"/no\"):\n",
    "            img = cv2.imread(data_path + \"/no/\" + file)\n",
    "            kp, desc = sift.detectAndCompute(img, None)\n",
    "            features = np.resize(desc, (256, 256))\n",
    "            label = np.array([0])\n",
    "            data.append([label, features])\n",
    "\n",
    "        train, test = train_test_split(data, test_size= 0.1)\n",
    "        train, val = train_test_split(train, test_size=0.11)\n",
    "\n",
    "        self.train_set = SIFTDataset(train)\n",
    "        self.test_set = SIFTDataset(test)\n",
    "        self.val_set = SIFTDataset(val)\n",
    "\n",
    "        print(\"SIFT Datamodule: ...complete!\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=self.batch_size, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "train_loader = SIFTDataModule()\n",
    "trainer = pl.Trainer()\n",
    "model = SIFTClf(in_channels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIFT DataModule: setup...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name     | Type       | Params\n",
      "----------------------------------------\n",
      "0 | accuracy | Accuracy   | 0     \n",
      "1 | model    | Sequential | 1.6 M \n",
      "----------------------------------------\n",
      "1.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 M     Total params\n",
      "6.426     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIFT Datamodule: ...complete!\n",
      "Epoch 22:   0%|          | 0/9 [00:00<?, ?it/s, loss=0.482, v_num=64]        "
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb00daff169136c9ed289eb7c835728e052c24a74f2ba1f56c431870fa98bf22"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml4h')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
